<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AuthentiVision: Multi-Modal Feature Integration and Attention Mechanisms for Robust Real vs. AI-Generated Face Identification</title>
    <style>
        /* Global styles */
        body {
            font-family: Arial, sans-serif;
            line-height: 1.8;
            margin: 0;
            padding: 0;
            background-color: #faf9f6;
            color: #2c3e50;
        }

        /* Container for content */
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
            background-color: white;
            box-shadow: 0 0 20px rgba(0,0,0,0.05);
        }

        /* Typography */
        h1, h2, h3, h4 {
            font-family: Georgia, 'Times New Roman', Times, serif;
            color: #2c3e50;
            line-height: 1.4;
            margin-top: 1.5em;
            margin-bottom: 0.8em;
        }

        h1 {
            font-size: 2.5rem;
            text-align: center;
            margin-bottom: 1.5em;
            line-height: 1.3;
        }

        h2 {
            font-size: 1.8rem;
            border-bottom: 2px solid #eee;
            padding-bottom: 0.3em;
        }

        h3 {
            font-size: 1.4rem;
            color: #34495e;
        }

        /* Links */
        a {
            color: #3498db;
            text-decoration: none;
            transition: color 0.3s ease;
        }

        a:hover {
            color: #2980b9;
            text-decoration: underline;
        }

        /* Header section */
        header {
            text-align: center;
            margin-bottom: 60px;
            padding: 20px;
            background: linear-gradient(to bottom, #fff, #faf9f6);
        }

        .author-list {
            list-style: none;
            padding: 0;
            margin: 20px 0;
        }

        .author-list li {
            display: inline-block;
            margin: 0 15px;
            font-size: 1.1rem;
        }

        .contact-info {
            margin-top: 20px;
            font-size: 0.95rem;
        }

        /* Content sections */
        .content-section {
            margin-bottom: 50px;
            padding: 0 20px;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            margin: 20px 0;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        /* Lists */
        ul, ol {
            padding-left: 25px;
            margin: 15px 0;
        }

        li {
            margin-bottom: 8px;
        }

        /* References section */
        .references {
            list-style-type: decimal;
            padding-left: 25px;
            font-size: 0.95rem;
        }

        .references li {
            margin-bottom: 15px;
        }

        /* Footer */
        footer {
            text-align: center;
            padding: 40px 0;
            margin-top: 60px;
            border-top: 1px solid #eee;
            color: #7f8c8d;
            font-size: 0.9rem;
        }

        /* Abstract section */
        .abstract {
            background-color: #f8f9fa;
            padding: 25px;
            border-radius: 5px;
            margin: 30px 0;
            border-left: 4px solid #3498db;
        }

        /* Responsive design */
        @media (max-width: 768px) {
            .container {
                padding: 20px 15px;
            }

            h1 {
                font-size: 2rem;
            }

            h2 {
                font-size: 1.5rem;
            }

            .author-list li {
                display: block;
                margin: 10px 0;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>AuthentiVision: Finding Yourself in the Real World</h1>

            <div class="contact-info">
                <p>
                    <a href="https://github.com/Time-machine/DeepAuthenFace" target="_blank">GitHub</a> |
                    <a href="https://huggingface.co/Time-machine/DeepAuthenFace" target="_blank">Hugging Face</a> |
                    <a href="https://huggingface.co/Time-machine/DeepAuthenFace" target="_blank">Data</a> |
                    <a href="mailto:haijian.wang@example.com">wanghaijian05@gmail.com</a>
                </p>
            </div>
        </header>

        <section class="content-section">
            <h2>Abstract</h2>
            <p>
                With the rapid advancement of Generative Adversarial Networks (GANs) and diffusion models, AI-generated face images have become visually indistinguishable from real faces, posing potential risks to social security and privacy protection. This paper presents <strong>DeepAuthenFace</strong>, an advanced face detection model that leverages multi-modal feature integration and attention mechanisms to accurately differentiate between authentic and AI-generated faces. The model employs a combination of high-level semantic features, texture analysis, frequency domain characteristics, edge information, and local binary patterns, which are meticulously fused using attention-based neural networks to enhance classification performance. Comprehensive experiments demonstrate the efficacy of DeepAuthenFace in achieving robust and high-precision face authenticity detection.
            </p>
        </section>

        <section class="content-section">
            <h2>1. Introduction</h2>
            <p>
                The proliferation of AI-generated imagery, particularly human faces, has significant implications for various domains, including security, privacy, and digital forensics. Traditional face detection systems primarily focus on recognizing facial features and expressions, often overlooking subtle artifacts introduced by generative models. To address this challenge, we propose DeepAuthenFace, a sophisticated model designed to discern the authenticity of facial images by integrating multiple feature modalities and employing attention mechanisms to prioritize critical information.
            </p>
        </section>

        <section class="content-section">
            <h2>2. Model Architecture</h2>
            <img src="assets/img_1.png" alt="Model Architecture Diagram">
            <h3>2.1. Backbone Network (EfficientNetV2-B2)</h3>
            <p>
                The foundation of DeepAuthenFace is built upon the pre-trained EfficientNetV2-B2 model, known for its exceptional performance in image classification tasks. EfficientNetV2-B2 provides a strong backbone for feature extraction due to its optimized architecture and scaling strategies. In our model, we leverage the high-level semantic features extracted by EfficientNetV2-B2, which outputs a 1408-dimensional feature vector. To prevent overfitting and improve training efficiency, we freeze the initial convolutional layers (<code>conv_stem</code>) and the first batch normalization layer (<code>bn1</code>). This allows the model to focus on learning representations that are most relevant for distinguishing between real and AI-generated faces, rather than relearning basic image features.
            </p>

            <h3>2.2. Gray-Level Co-occurrence Matrix (GLCM) Features</h3>
            <p>
                To capture texture-related information that may differ between real and AI-generated images, we employ the Gray-Level Co-occurrence Matrix (GLCM). GLCM is a statistical method that considers the spatial relationship of pixels, providing valuable insights into the texture patterns within an image. In our approach, we first convert the grayscale image to a 64-level quantized image to simplify computations. This quantization reduces the range of gray levels, making the GLCM computation more tractable.
            </p>
            <p>
                We then compute the GLCM for the quantized image at four different angles (0째, 45째, 90째, and 135째) with a pixel distance of 1. This captures texture patterns in multiple orientations, which is crucial for identifying subtle differences in texture that may indicate whether an image is real or AI-generated. From the computed GLCM, we extract several statistical properties: contrast, dissimilarity, homogeneity, energy, and correlation. These properties describe various aspects of the texture, such as the contrast between neighboring pixels, the uniformity of the texture, and the linear dependencies in the image.
            </p>
            <p>
                The extracted features from all angles and properties are concatenated to form a 20-dimensional feature vector. This vector effectively summarizes the texture information of the image and serves as an important input to our model for distinguishing between authentic and AI-generated faces.
            </p>

            <h3>2.3. Spectral Features</h3>
            <p>
                Analyzing the frequency domain of images can reveal artifacts introduced by AI generation methods that may not be apparent in the spatial domain. To exploit this, we perform a spectral analysis using the Fast Fourier Transform (FFT) on the grayscale images. The FFT transforms the image into its frequency components, allowing us to examine the magnitude spectrum. We apply a logarithmic scale to the magnitude spectrum to enhance contrast and avoid issues with very small or zero values.
            </p>
            <p>
                To capture the global frequency characteristics, we compute the radial average of the magnitude spectrum. This involves calculating the average magnitude at different distances from the center of the frequency domain image. The result is a fixed-length feature vector (181-dimensional in our case) that represents the distribution of frequency components in the image. These spectral features can help the model detect patterns or anomalies in the frequency domain that are indicative of AI-generated images.
            </p>

            <h3>2.4. Edge Features</h3>
            <p>
                Edges in images convey important structural information, and discrepancies in edge patterns can be indicative of AI-generated content. We extract edge features using the Canny edge detection algorithm, which is effective in identifying sharp changes in intensity. The process begins by converting the grayscale image to an 8-bit unsigned integer format and applying the Canny algorithm to obtain a binary edge map.
            </p>
            <p>
                To manage the dimensionality of the edge features, we resize the edge map to a fixed size of 64x64 pixels. This ensures consistency across all images and reduces computational load. The resized edge map is then used as input to a series of convolutional layers, which extract high-level edge features that capture the structural nuances of the image. These features are crucial for detecting irregularities in edges that may result from AI image generation processes.
            </p>

            <h3>2.5. Local Binary Patterns (LBP) Features</h3>
            <p>
                Local Binary Patterns (LBP) are a powerful means of summarizing local texture in images. LBP works by thresholding the neighborhood of each pixel and considering the result as a binary number. In our model, we use LBP with a radius of 1 and 8 sampling points, operating in 'uniform' mode. This configuration effectively captures micro-patterns in the texture of the image.
            </p>
            <p>
                After computing the LBP image, we generate a histogram of the patterns, which results in a 10-dimensional feature vector due to the 'uniform' setting. This histogram represents the frequency of different local texture patterns within the image. Since AI-generated images may have subtle inconsistencies in texture, the LBP features help our model to detect these variations and contribute to accurate classification.
            </p>
        </section>

        <section class="content-section">
            <h2>3. Attention Mechanism</h2>
            <p>
                To enhance the model's focus on critical features, we incorporate an attention mechanism after each feature extraction branch. The <code>AttentionBlock</code> consists of two fully connected layers with a bottleneck architecture, reducing the feature dimension to one-eighth of its original size before expanding it back. This structure allows the network to learn non-linear relationships and assign weights to different elements of the feature vectors. The attention weights are applied to the features, amplifying important components while suppressing less relevant ones. This mechanism enables the model to dynamically prioritize features that are most indicative of the authenticity of the face images.
            </p>
        </section>

        <section class="content-section">
            <h2>4. Feature Fusion and Classification</h2>
            <p>
                After processing through their respective attention modules, the feature vectors from all branches are concatenated to form a comprehensive fusion feature vector. Specifically, we combine the image features (1408-dimensional), GLCM features (64-dimensional), spectral features (64-dimensional), edge features (2048-dimensional), and LBP features (64-dimensional), resulting in a 3648-dimensional vector. This fusion vector encapsulates diverse information from multiple modalities, providing a rich representation for classification.
            </p>
            <p>
                To effectively integrate these features and reduce the dimensionality, we employ a fusion layer composed of fully connected layers with batch normalization, ReLU activation functions, and dropout for regularization. The fusion layer learns complex interactions among the different feature modalities, enabling the model to capture subtle patterns that may not be apparent when considering each feature type in isolation.
            </p>
            <p>
                The output layer produces a single scalar value, which is transformed into a probability score using the sigmoid function. This score represents the likelihood that the input image is AI-generated. By applying a threshold (typically 0.5), we can classify images as real or AI-generated. This approach allows the model to make nuanced decisions based on the combined information from all feature modalities.
            </p>
        </section>

        <section class="content-section">
            <h2>5. Training Procedure</h2>
            <p>
                The training process is meticulously designed to optimize the model's performance while preventing overfitting. We utilize the binary cross-entropy loss function with logits (<code>BCEWithLogitsLoss</code>), which is appropriate for binary classification tasks. The Adam optimizer is employed with an initial learning rate of 1e-4 and a weight decay of 1e-5 to ensure stable and efficient convergence.
            </p>
            <p>
                To adjust the learning rate dynamically, we implement a scheduler (<code>ReduceLROnPlateau</code>) that reduces the learning rate by a factor of 0.5 if the validation accuracy does not improve for three consecutive epochs. This helps the model to escape local minima and converge to a better solution. An early stopping mechanism is also incorporated, which terminates the training if the validation accuracy does not improve for ten consecutive epochs, thus preventing overfitting.
            </p>
            <img src="assets/img_22.png" alt="Training Metrics">
            <p>
                To enhance computational efficiency, we leverage mixed precision training using <code>torch.cuda.amp</code>, which accelerates computations and reduces memory usage by performing operations in half-precision where appropriate. Data augmentation techniques are extensively applied to improve the model's generalization capabilities. These include random horizontal flips, rotations, color jittering, affine transformations, and perspective distortions, which introduce variability in the training data and help the model to be robust against diverse image conditions.
            </p>
        </section>


        <section class="content-section">
            <h2>6. Results and Performance</h2>
            <p>
                DeepAuthenFace demonstrates exceptional performance in distinguishing between real and AI-generated faces. The integration of multi-modal features allows the model to capture a wide range of information, from high-level semantic content to low-level texture and frequency details. The attention mechanisms effectively prioritize the most discriminative features, enhancing the model's ability to detect subtle differences.
            </p>
            <p>
                In extensive experiments, the model achieved high accuracy and robustness across diverse datasets. The use of advanced training techniques, such as dynamic learning rate adjustment and extensive data augmentation, contributed to its strong generalization capabilities. The results validate the effectiveness of our approach in addressing the challenges posed by realistic AI-generated face images.
            </p>

        </section>

        <section class="content-section">
            <h2>7. Conclusion</h2>
            <p>
                DeepAuthenFace leverages the synergy of multi-feature integration and attention-based neural networks to deliver a highly accurate and robust solution for face authenticity detection. The model's comprehensive feature extraction and sophisticated attention mechanisms enable it to effectively differentiate between real and AI-generated faces, addressing critical challenges in security and privacy domains. Future work will explore the incorporation of additional feature extraction techniques and further refinement of attention mechanisms to enhance model performance and generalization.
            </p>
        </section>

        <footer>
            <p>&copy; 2024 DeepAuthenFace Project Team</p>
        </footer>

    </div>
</body>
</html>
